{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/telvid/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 256\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "EMBEDDING_MODEL = \"intfloat/e5-large\"\n",
    "\n",
    "K = 2\n",
    "\n",
    "LLM = \"google/gemma-2-9b-it\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI visual assistant surveillance operator that can analyze real-time traffic analysis and accident detection.\n",
    "\n",
    "Respond to user's questions as accurately as possible.\n",
    "Be careful not to answer with false information.\n",
    "\n",
    "Using the provided caption information, describe the scene in a detailed manner.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "QUANTIZATION = \"bf16\" # \"qlora\", \"bf16\", \"fp16\"\n",
    "\n",
    "MAX_NEW_TOKENS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(file_path, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    loader = JSONLoader(\n",
    "        file_path=file_path,\n",
    "        jq_schema=\".frame.[].caption\",\n",
    "        text_content=False,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    chunks = docs.copy()\n",
    "    return chunks\n",
    "\n",
    "def create_vector_db(chunks, model_path=EMBEDDING_MODEL):\n",
    "    \"\"\"FAISS DB\"\"\"\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "    \n",
    "def process_jsons_from_dataframe(unique_paths, base_directory):\n",
    "    \n",
    "    json_databases = {}\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing JSONs\"):\n",
    "        \n",
    "        normalized_path = unicodedata.normalize('NFC', path)\n",
    "        full_path = os.path.normpath(\n",
    "            os.path.join(\n",
    "                base_directory, normalized_path.lstrip('./')\n",
    "            )\n",
    "        ) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        json_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        \n",
    "        print(f\"Processing {json_title}...\")\n",
    "        \n",
    "        chunks = process_json(full_path)\n",
    "        db = create_vector_db(chunks)\n",
    "        \n",
    "        # Retriever\n",
    "        \n",
    "        retriever_similarity = db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={'k': K}\n",
    "        )\n",
    "\n",
    "        retriever_mmr = db.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={'k': K}\n",
    "        )\n",
    "\n",
    "        retriever_bm25 = BM25Retriever.from_documents(chunks)\n",
    "        \n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[retriever_similarity, retriever_mmr, retriever_bm25],\n",
    "            weights=[0.5, 0.5, 0.5]\n",
    "        )        \n",
    "        \n",
    "        json_databases[json_title] = {\n",
    "                'db': db,\n",
    "                'retriever': retriever\n",
    "        }\n",
    "    return json_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cam_07.mp4-meta_db.json': 'cam_07.mp4-meta_db.json',\n",
       " 'demo_1.mp4-verb.json': 'demo_1.mp4-verb.json'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_directory = '/home/jiyul/SPS_JY/TeletoVision_demo/models/data/' # Your Base Directory\n",
    "db_list = {filename: filename for filename in os.listdir(base_directory)}\n",
    "db_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cam_07.mp4-meta_db...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONs: 100%|██████████| 1/1 [00:04<00:00,  4.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# base_directory = 'data/' # Your Base Directory\n",
    "unique_paths = [db_list['cam_07.mp4-meta_db.json']]\n",
    "json_databases = process_jsons_from_dataframe(unique_paths, base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline():\n",
    "    model_id = LLM\n",
    "    quantization_options = {\n",
    "        \"qlora\": {\"quantization_config\": BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16)},\n",
    "        \"bf16\": {\"torch_dtype\": torch.bfloat16},\n",
    "        \"fp16\": {\"torch_dtype\": \"float16\"}\n",
    "    }\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        **quantization_options.get(QUANTIZATION, {})\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        # temperature=0.2,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        # do_sample=True,\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cam_07.mp4-meta_db': {'db': <langchain_community.vectorstores.faiss.FAISS at 0x7fd6589e4a30>,\n",
       "  'retriever': EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fd6589e4a30>, search_kwargs={'k': 2}), VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fd6589e4a30>, search_type='mmr', search_kwargs={'k': 2}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x7fd658adc220>)], weights=[0.5, 0.5, 0.5])}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame : 22\n",
      "A man is lying on the ground in the middle of a busy street.\n",
      "=========================================================\n",
      "frame : 15\n",
      "A motorcyclist is laying on the ground in the middle of the street.\n",
      "=========================================================\n",
      "frame : 17\n",
      "A motorcycle rider is laying on the ground in the middle of the street.\n",
      "=========================================================\n",
      "frame : 24\n",
      "A motorcycle rider is laying on the ground with his motorcycle on its side.\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "prompt_sample = 'person lying on the ground'\n",
    "source = 'cam_07.mp4-meta_db'\n",
    "\n",
    "docs = json_databases[source]['retriever'].invoke(prompt_sample)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"frame : {doc.metadata['seq_num']-1}\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain 을 이용한 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Find the frame with a person lying on the floor\n",
      "Answer: Frames 16, 17, 22, and 24 all show a person lying on the floor. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# docs = json_databases[source]['retriever'].invoke(prompt_sample)\n",
    "\n",
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += f\"frame : {doc.metadata['seq_num']-1}\"\n",
    "        context += '\\n'\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context\n",
    "\n",
    "results = []\n",
    "\n",
    "question = 'Find the frame with a person lying on the floor'\n",
    "source = 'cam_07.mp4-meta_db'\n",
    "\n",
    "retriever = json_databases[source]['retriever']\n",
    "\n",
    "template = PROMPT_TEMPLATE\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# RAG 체인 정의\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "full_response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "results.append({\n",
    "    \"Question\": question,\n",
    "    \"Answer\": full_response\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telvid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
